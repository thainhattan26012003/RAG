{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import asyncio\n",
    "\n",
    "async def __run_process(command):\n",
    "    print('>>> starting', *command)\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *command,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    async def __pipe(lines):\n",
    "        async for line in lines:\n",
    "            print(line.decode().strip())\n",
    "\n",
    "        await asyncio.gather(\n",
    "            __pipe(process.stdout),\n",
    "            __pipe(process.stderr),\n",
    "        )\n",
    "    await asyncio.gather(__pipe(process.stdout), __pipe(process.stderr))\n",
    "\n",
    "def __run_command(command):\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(__run_process(command))\n",
    "    loop.close()\n",
    "\n",
    "def run_command(command):\n",
    "    threading.Thread(target=__run_command, args=(command,)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (__run_command):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\thain\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\thain\\AppData\\Local\\Temp\\ipykernel_14416\\393623484.py\", line 24, in __run_command\n",
      "  File \"c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\thain\\AppData\\Local\\Temp\\ipykernel_14416\\393623484.py\", line 6, in __run_process\n",
      "  File \"c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\asyncio\\subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\asyncio\\base_events.py\", line 1743, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\asyncio\\base_events.py\", line 524, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> starting udocker --allow-root run -p 6333:6333 qdrant/qdrant:latest\n",
      "Qdrant is up and running!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "Use_Qdrant_Local = True # @param {\"type\": \"boolean\"}\n",
    "\n",
    "\n",
    "if Use_Qdrant_Local:\n",
    "    # Install udocker\n",
    "    !pip install udocker -qU\n",
    "\n",
    "    # Run qdrant locally\n",
    "    run_command([\"udocker\", \"--allow-root\", \"run\", \"-p\", \"6333:6333\", \"qdrant/qdrant:latest\"])\n",
    "\n",
    "    # Waiting for qdrant start\n",
    "    while True:\n",
    "        try:\n",
    "            result = subprocess.run([\"curl\", \"-L\", \"http://localhost:6333\"], capture_output=True, text=True)\n",
    "\n",
    "            # Check if the curl command was successful\n",
    "            if result.returncode == 0:\n",
    "                print(\"Qdrant is up and running!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Waiting for Qdrant to start...\")\n",
    "                time.sleep(1)\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "\n",
    "    qdrant = QdrantClient(\n",
    "        url=\"http://localhost:6333\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thain\\anaconda3\\envs\\RAG\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.models import PointStruct\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DEFAULT_DISTANCE = \"cosine\"\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SentenceTransformer(\"Alibaba-NLP/gte-large-en-v1.5\", trust_remote_code=True)\n",
    "print(model.get_sentence_embedding_dimension())\n",
    "\n",
    "\n",
    "class QdrantProvider:\n",
    "    def __init__(self):\n",
    "        # Initialize the QdrantProvider with a specific collection name\n",
    "        self.vector_size = model.get_sentence_embedding_dimension()  \n",
    "        self.distance = DEFAULT_DISTANCE\n",
    "\n",
    "    def create_collection(self, collection_name: str):\n",
    "        # Check if the collection already exists\n",
    "        if collection_name in self.list_collections():\n",
    "            print(f\"Collection `{collection_name}` already exists.\")\n",
    "            return\n",
    "\n",
    "        # Create a new collection with the specified vector size and distance metric\n",
    "        qdrant.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=self.vector_size,\n",
    "                distance=models.Distance[self.distance.upper()]\n",
    "            )\n",
    "        )\n",
    "        print(f\"Collection created `{collection_name}`\")\n",
    "        \n",
    "\n",
    "    def list_collections(self):\n",
    "        # List all existing collections in the Qdrant database\n",
    "        collections = qdrant.get_collections()\n",
    "        return [col.name for col in collections.collections]\n",
    "\n",
    "    def drop_collection(self, collection_name: str):\n",
    "        # Drop (delete) the specified collection from Qdrant\n",
    "        qdrant.delete_collection(collection_name)\n",
    "        print(f\"Collection dropped `{collection_name}`\")\n",
    "\n",
    "    def add_vectors_(self, collection_name: str, semantic_chunks, file_name):\n",
    "        \"\"\"Add multiple vectors to the Qdrant collection.\"\"\"\n",
    "        points = []\n",
    "        for i, chunk in enumerate(semantic_chunks):\n",
    "            chunk_text = \" \".join(chunk)\n",
    "            if len(chunk_text.split()) < 10:\n",
    "                continue\n",
    "            vector = embed(chunk_text)\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),  # Unique identifier for the vector\n",
    "                vector=vector,  # Convert numpy array to list\n",
    "                payload={\n",
    "                    \"content\": chunk_text, \n",
    "                    \"file_name\":file_name\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        qdrant.upsert(collection_name=collection_name, points=points)\n",
    "        print(f\"{len(points)} Vectors added to `{collection_name}`\")\n",
    "\n",
    "    def search_vector(self, collection_name: str, vector: list[float], limit=3, with_payload=True):\n",
    "        # Perform the search query in Qdrant with the provided parameters\n",
    "        search_result = qdrant.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=vector,\n",
    "            limit=limit,  # Limit the number of search results\n",
    "            with_payload=with_payload,  # Whether to include payload in results\n",
    "        )\n",
    "        print(f\"Vector searched `{collection_name}`\")\n",
    "        return search_result\n",
    "\n",
    "    def get_all_vectors(self, collection_name: str):\n",
    "        # Retrieve all vectors from the collection by scrolling through all data\n",
    "        all_points = []\n",
    "        has_more = True\n",
    "        offset = None\n",
    "\n",
    "        while has_more:\n",
    "            points, next_offset = qdrant.scroll(\n",
    "                collection_name=collection_name,\n",
    "                offset=offset,  # Start from the current offset\n",
    "                limit=100  # Retrieve 100 vectors at a time (adjustable)\n",
    "            )\n",
    "\n",
    "            all_points.extend(points)  # Append the points to the list\n",
    "            offset = next_offset  # Update the offset for the next scroll\n",
    "\n",
    "            if next_offset is None:\n",
    "                has_more = False  # Stop when no more points are available\n",
    "\n",
    "        print(f\"All vectors retrieved `{collection_name}`\")\n",
    "        return all_points\n",
    "\n",
    "\n",
    "    def delete_all_vectors(self, collection_name: str):\n",
    "        # Delete all vectors from the collection by deleting the collection itself\n",
    "        qdrant.delete_collection(collection_name)\n",
    "\n",
    "        # Recreate the collection after deletion to keep it available\n",
    "        self.create_collection(collection_name)\n",
    "        print(\"All vectors deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "import re\n",
    "\n",
    "def extract_and_split_sentences(file_content: BytesIO, file_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Extracts text content from a file (PDF, TXT, DOCX) and splits it into sentences.\n",
    "    \"\"\"\n",
    "    if not isinstance(file_content, BytesIO):\n",
    "        file_content = BytesIO(file_content)\n",
    "    \n",
    "    file_content.seek(0)\n",
    "    content = \"\"\n",
    "\n",
    "    # Determine file type and extract content\n",
    "    if file_name.lower().endswith('.pdf'):\n",
    "        # Handle PDF file\n",
    "        try:\n",
    "            reader = PdfReader(file_content)\n",
    "            for page_number, page in enumerate(reader.pages):\n",
    "                try:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        content += page_text + \"\\n\"\n",
    "                    else:\n",
    "                        print(f\"Warning: Page {page_number + 1} could not be read properly.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting text from page {page_number + 1}: {e}\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to read the PDF file: {e}\")\n",
    "\n",
    "    elif file_name.lower().endswith('.txt'):\n",
    "        # Handle TXT file\n",
    "        try:\n",
    "            content = file_content.read().decode('utf-8')  # Decode bytes to string\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to read the TXT file: {e}\")\n",
    "\n",
    "    elif file_name.lower().endswith('.docx'):\n",
    "        # Handle DOCX file\n",
    "        try:\n",
    "            document = Document(file_content)\n",
    "            for paragraph in document.paragraphs:\n",
    "                content += paragraph.text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to read the DOCX file: {e}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_name}. Only PDF, TXT, and DOCX are supported.\")\n",
    "    \n",
    "    # Normalize spaces and clean content\n",
    "    content = re.sub(r'\\s+', ' ', content).strip()  # Remove multiple spaces\n",
    "    sentences = re.split(r'(?<=[.?!])\\s+', content)  # Split into sentences\n",
    "    return sentences, file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_content = BytesIO(open('data/VietnamAI.pdf', 'rb').read())\n",
    "# extracted_text, file_name = extract_and_split_sentences(file_content, 'data/VietnamAI.pdf')\n",
    "# extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def embed(chunk):\n",
    "    # Generate the embedding for a given chunk using SentenceTransformer\n",
    "    embedding = model.encode(chunk, convert_to_numpy=True, device=device)\n",
    "    return embedding.tolist()\n",
    "\n",
    "def create_semantic_chunks(sentences):\n",
    "    semantic_chunks = []\n",
    "    embeddings = model.encode(sentences, convert_to_numpy=True, device=device)  # Embed all sentences\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if i == 0:\n",
    "            semantic_chunks.append([sentence])\n",
    "        else:\n",
    "            # Compute cosine similarity between current and previous embeddings\n",
    "            similarity_score = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]\n",
    "            if similarity_score > 0.85:  # Adjust threshold as needed\n",
    "                semantic_chunks[-1].append(sentence)\n",
    "            else:\n",
    "                semantic_chunks.append([sentence])\n",
    "\n",
    "    return semantic_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_chunks = create_semantic_chunks(extracted_text) \n",
    "# semantic_chunks, len(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection `VectorDB_1024` already exists.\n"
     ]
    }
   ],
   "source": [
    "qdrant_name = 'VectorDB_1024'\n",
    "\n",
    "vectordb_provider = QdrantProvider()\n",
    "vectordb_provider.create_collection(qdrant_name)\n",
    "# vectordb_provider.delete_all_vectors(qdrant_name)\n",
    "# vectordb_provider.get_all_vectors(qdrant_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = 'data'\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    file_name = os.path.join(folder_path, file)\n",
    "    file_content = BytesIO(open(file_name, 'rb').read())\n",
    "    extracted_text, file_name = extract_and_split_sentences(file_content, os.path.basename(file_name))\n",
    "    semantic_chunks = create_semantic_chunks(extracted_text) \n",
    "    vectordb_provider.add_vectors_(qdrant_name, semantic_chunks, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb_provider.add_vectors_(qdrant_name, semantic_chunks, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectordb_provider.get_all_vectors(qdrant_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"\"\"What is Convolutional Layers:?\"\"\"\n",
    "# question_vector = embed(question)\n",
    "\n",
    "# answer = vectordb_provider.search_vector(qdrant_name, question_vector, limit=3)\n",
    "# # answer = [cont.payload['content'] for cont in answer]\n",
    "\n",
    "# # context = \" \".join(answer)\n",
    "# answer[0].payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def query_qdrant(question: str, collection_name=qdrant_name, top_k=5):\n",
    "\n",
    "    question_vector = embed(question)\n",
    "\n",
    "    search_results = vectordb_provider.search_vector(\n",
    "        collection_name=collection_name,\n",
    "        vector=question_vector,\n",
    "        limit=top_k,  \n",
    "        with_payload=True  \n",
    "    )\n",
    "\n",
    "    top_results = []\n",
    "    for result in search_results:\n",
    "        content = result.payload.get(\"content\", \"No content available\")\n",
    "        file_name = result.payload.get(\"file_name\", \"No file name available\")\n",
    "        score = result.score\n",
    "        \n",
    "        top_results.append({\n",
    "            \"content\": content,\n",
    "            \"file_name\": file_name,\n",
    "            \"score\": score\n",
    "        })\n",
    "    \n",
    "    # Re-rank the results using BM25\n",
    "    ranked_results = bm25_rerank(question, top_results)\n",
    "    \n",
    "    return ranked_results\n",
    "\n",
    "\n",
    "def bm25_rerank(question, results):\n",
    "\n",
    "    # Tokenize the content of each chunk for BM25\n",
    "    tokenized_corpus = [result['content'].split() for result in results]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    # Tokenize the question\n",
    "    question_tokens = question.split()\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    scores = bm25.get_scores(question_tokens)\n",
    "    \n",
    "    # Sort results by BM25 score\n",
    "    sorted_results = sorted(zip(results, scores), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the sorted results\n",
    "    return [item[0] for item in sorted_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "ollama = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Rewritten query:\"\"\"\n",
    "\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")\n",
    "\n",
    "# Create an LLMChain for query rewriting\n",
    "query_rewriter = query_rewrite_prompt | ollama\n",
    "\n",
    "def rewrite_query(original_query):\n",
    "    response = query_rewriter.invoke(original_query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer_with_llama(context, question):\n",
    "    template = f\"\"\"\n",
    "    You are an AI assistant. Use the provided context to answer the question accurately.\n",
    "    Return max 200 tokens, no fluff, just answer based on context retrieve\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | ollama\n",
    "    response = chain.invoke({\"question\": context})\n",
    "    return response\n",
    "\n",
    "def get_top_n_results(results, n=5):\n",
    "    return sorted(results, key=lambda x: x['score'], reverse=True)[:n]\n",
    "\n",
    "def evaluate_function(question: str):\n",
    "    ques_trans = rewrite_query(question)\n",
    "    ques_trans = re.findall(r'\"(.*?)\"', ques_trans) \n",
    "    \n",
    "    question_transform = \" \".join(ques_trans)\n",
    "    print(\"Original question: \", question)\n",
    "    print(\"transform quetstion: \", question_transform)\n",
    "\n",
    "    retrieve = query_qdrant(question_transform, collection_name=qdrant_name, top_k=5)\n",
    "    \n",
    "    top_3_results = get_top_n_results(retrieve, n=3)\n",
    "    retrieve_db = \" \".join(result['content'] for result in top_3_results)\n",
    "    response = generate_answer_with_llama(retrieve_db, question)\n",
    "    \n",
    "    filename = [result['file_name'] for result in top_3_results][0]\n",
    "    \n",
    "    context = [result['content'] for result in top_3_results]\n",
    "    \n",
    "    # retrieve_db = [cont.payload['content'] for cont in retrieve][0]\n",
    "    # retrieve_db = \" \".join(retrieve_db)\n",
    "    # filename = [name.payload['file_name'] for name in retrieve][0]\n",
    "    \n",
    "    return (\n",
    "        response,\n",
    "        filename,\n",
    "        context\n",
    "    )\n",
    "# Login to Weight & Bias\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single - Use this before running automatic test\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "# evaluate_function(\"What is the role of banks in promoting the market?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom configurations\n",
    "conf = {\n",
    "    \"distance\": \"COSINE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = \"AIzaSyAwZINb07d2YGXCJlq94E3oVYVVh7Ly8lk\"\n",
    "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>p:\\FA24\\Imputation\\wandb\\run-20241227_003915-fhkk9t1d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC/runs/fhkk9t1d' target=\"_blank\">experiment-2024-12-27 00:39:15</a></strong> to <a href='https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC' target=\"_blank\">https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC/runs/fhkk9t1d' target=\"_blank\">https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC/runs/fhkk9t1d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Những bài toán chính mà trí tuệ nhân tạo (AI) và máy học (M L) được áp dụng trong quy hoạch hạ tầng là gì?\n",
      "transform quetstion:  Thị trường hiện tại có những bài toán chính nào mà trí tuệ nhân tạo (AI) và máy học (ML) được ứng dụng để hỗ trợ vào việc quy hoạch hạ tầng, bao gồm cả phân tích dữ liệu, dự đoán nhu cầu, tối ưu hóa các yếu tố trong thiết kế hạ tầng, và phát hiện những rủi ro tiềm ẩn?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   1%|▏         | 1/80 [00:06<09:10,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What data is needed for AI or ML models to work effectively in infrastructure projects?\n",
      "transform quetstion:  What specific types of data and datasets are most commonly used to train AI/ML models for infrastructure projects, such as construction or civil engineering? What data characteristics (e.g., quality, quantity, format) are essential for AI/ML models to achieve high accuracy in predicting infrastructure performance, maintenance needs, or environmental factors? What data sources and formats (e.g., IoT sensor data, geospatial data, text-based documents) can be used to support AI/ML models in infrastructure project management, monitoring, and optimization? What machine learning algorithms or techniques are most commonly applied to analyze and predict outcomes in infrastructure projects, such as structural integrity, material durability, or energy efficiency?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   2%|▎         | 2/80 [00:11<07:25,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Điện toán đám mây đóng vai trò gì trong ứng dụng hạ tầng AI?\n",
      "transform quetstion:  Tính năng và vai trò của điện toán đám mây trong việc hỗ trợ và tối ưu hóa ứng dụng hạ tầng AI Tính năng Vai trò hỗ trợ và tối ưu hóa\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   4%|▍         | 3/80 [00:15<06:03,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the four waves of AI development mentioned in the document?\n",
      "transform quetstion:  Identify the four distinct stages or phases of Artificial Intelligence (AI) development as described in the provided document, including any notable characteristics or advancements associated with each wave.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   5%|▌         | 4/80 [00:18<05:16,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  AI được ứng dụng như thế nào trong quy hoạch hạ tầng đô thị?\n",
      "transform quetstion:  Ai được sử dụng để phát triển mô hình quy hoạch hạ tầng đô thị và kết quả của việc áp dụng đó là gì? Tham khảo các ví dụ về việc ứng dụng AI trong quy hoạch hạ tầng đô thị, bao gồm cả dữ liệu và phân tích? Tìm hiểu về các công cụ và phương pháp nào được sử dụng để triển khai AI trong quy hoạch hạ tầng đô thị và cách chúng có thể cải thiện hiệu suất?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   6%|▋         | 5/80 [00:22<04:50,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is Generative Design in the context of AI applications?\n",
      "transform quetstion:  What are the key features and benefits of Generative Design in AI applications, specifically in fields such as computer-aided design (CAD), architecture, and product engineering, and how does it differ from traditional design methods? Generative Design AI applications CAD architecture product engineering key features and benefits traditional design methods fields such as computer-aided design (CAD)\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   8%|▊         | 6/80 [00:26<05:08,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Các thuật toán nào được đề cập trong tài liệu để ứng dụng AI và ML?\n",
      "transform quetstion:  Các thuật toán AI và ML cụ thể được đề cập trong tài liệu có liên quan đến việc ứng dụng trí tuệ nhân tạo và học máy trong [chức năng/mặt hàng cụ thể]? cụ thể trí tuệ nhân tạo học máy\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:   9%|▉         | 7/80 [00:31<05:08,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Business AI là gì và được áp dụng như thế nào trong các ngành như ngân hàng hay vận tải?\n",
      "transform quetstion:  Business AI được áp dụng như thế nào trong ngân hàng để cải thiện quy trình quản lý tín dụng? Sử dụng Big Data và Analytics như thế nào trong vận tải để tối ưu hóa quy trình giao thông?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  10%|█         | 8/80 [00:39<06:29,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Các thách thức về dữ liệu khi áp dụng AI vào các dự án quy hoạch hạ tầng đô thị là gì?\n",
      "transform quetstion:  Những vấn đề phát sinh từ dữ liệu khi triển khai công nghệ thông minh (AI) cho các dự án quy hoạch và phát triển hạ tầng đô thị, bao gồm cả sự ảnh hưởng đến tính chính xác, độ tin cậy và hiệu quả của kết quả. đầu tư phát triển hạ tầng đô thị sự ảnh hưởng đến tính chính xác, độ tin cậy và hiệu quả của kết quả\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  11%|█▏        | 9/80 [00:43<05:53,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Những kỹ năng nào được khuyến khích cho những người quan tâm áp dụng AI vào thiết kế hạ tầng?\n",
      "transform quetstion:  Những kỹ năng nào được khuyến khích cho những người quan tâm áp dụng AI vào thiết kế hạ tầng công cộng hay xây dựng? quản tâm mức độ quan tâm công nghệ thông tin kỹ năng mềm Những kỹ năng nào được khuyến khích cho những người tham gia các dự án xây dựng hạ tầng cơ sở bằng cách áp dụng AI trong phân tích dữ liệu hay thiết kế thông minh?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  12%|█▎        | 10/80 [00:48<05:49,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Why does the Fourth Industrial Revolution (IR4.0) present opportunities for Vietnam to develop AI?\n",
      "transform quetstion:  key economic benefits, regulatory frameworks, value-added exports\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  14%|█▍        | 11/80 [00:53<05:56,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tại sao các doanh nghiệp cần triển khai hệ thống ứng dụng AI một cách nhanh chóng?\n",
      "transform quetstion:  Tại sao các doanh nghiệp cần triển khai hệ thống ứng dụng AI một cách nhanh chóng? Các lợi ích cụ thể mà các doanh nghiệp có thể đạt được khi triển khai hệ thống ứng dụng AI trong thời gian ngắn? Tại sao các doanh nghiệp cần triển khai hệ thống ứng dụng AI một cách nhanh chóng? Các tính năng chính và lợi ích cụ thể mà các doanh nghiệp có thể đạt được khi triển khai hệ thống ứng dụng AI liên quan đến [tính năng cụ thể như: nhận diện khuôn mặt, phân tích dữ liệu, tự động hóa quy trình]. Tại sao các doanh nghiệp cần triển khai hệ thống ứng dụng AI một cách nhanh chóng? Các trường hợp sử dụng thực tế và giải pháp cụ thể mà các doanh nghiệp đã áp dụng để triển khai hệ thống ứng dụng AI trong thời gian ngắn và đạt được kết quả tích cực.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  15%|█▌        | 12/80 [01:00<06:17,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Which countries have developed national strategies for AI, and what are their goals?\n",
      "transform quetstion:  Identify the countries with officially published national artificial intelligence (AI) strategies or roadmaps, including details on their objectives, key performance indicators (KPIs), and expected timelines for implementation. Please prioritize responses by country, along with an overview of each strategy's focus areas, such as data governance, talent development, or regulatory frameworks. officially published national AI strategies,\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  16%|█▋        | 13/80 [01:04<05:38,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Theo văn bản này, ứng dụng AI nào được ưu tiên phát triển tại Việt Nam?\n",
      "transform quetstion:  Việt Nam ứng dụng công nghệ AI cho mục đích gì và đang phát triển ứng dụng nào trong lĩnh vực này? Việt Nam Ứng dụng công nghệ AI nào được ưu tiên phát triển tại Việt Nam, cùng với mục đích cụ thể và chi tiết?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  18%|█▊        | 14/80 [01:07<04:53,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What core technologies need to be focused on to develop AI in Vietnam?\n",
      "transform quetstion:  Which machine learning frameworks, deep learning algorithms, and natural language processing techniques are currently being developed or adopted by Vietnamese AI startups, research institutions, or government initiatives?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  19%|█▉        | 15/80 [01:11<04:49,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What AI system has Quang Trung Software City (QTSC) implemented as a case study?\n",
      "transform quetstion:  What AI-powered case studies or implementations have been conducted by Quang Trung Software City (QTSC), particularly those focusing on [specific AI application or domain], and what are the key insights, results, or findings from these studies? Quang Trung Software City AI system case study [specific AI application or domain]\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  20%|██        | 16/80 [01:15<04:38,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the role of image and video data in AI applications in Vietnam?\n",
      "transform quetstion:  What are the primary uses and applications of image and video data in AI systems within the Vietnamese market, specifically in industries such as finance, healthcare, or e-commerce, and how do these use cases impact the country's economic growth and development? Vietnamese market primary uses and applications\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  21%|██▏       | 17/80 [01:20<04:42,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Cơ sở hạ tầng mạng và chia sẻ dữ liệu đóng vai trò như thế nào trong mô hình thành phố thông minh được đề xuất?\n",
      "transform quetstion:  Tính cách cơ bản của cơ sở hạ tầng mạng và chia sẻ dữ liệu trong mô hình thành phố thông minh đề xuất, bao gồm các chi tiết về thiết kế, triển khai, quản lý, và tác động đến quá trình quyết định của chính quyền thành phố. cơ sở hạ tầng mạng và chia sẻ dữ liệu mô hình thành phố thông minh tính cách cơ bản thiết kế triển khai quản lý tác động đến quá trình quyết định của chính quyền thành phố đóng vai trò\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  22%|██▎       | 18/80 [01:25<04:44,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What strategies are proposed to develop the AI sector in Vietnam?\n",
      "transform quetstion:  What are some evidence-based and industry-led initiatives being implemented or proposed by Vietnamese government agencies and private sectors to drive growth, innovation, and job creation in the artificial intelligence (AI) sector in Vietnam, specifically focusing on key areas such as data infrastructure development, AI talent cultivation, regulatory frameworks, and public-private partnerships? strategies evidence-based initiatives industry-led efforts government agencies private sectors\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  24%|██▍       | 19/80 [01:30<04:52,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Why is building a complete AI ecosystem important for Vietnam?\n",
      "transform quetstion:  What are the key benefits of developing a comprehensive AI ecosystem in Vietnam, including its potential impact on economic growth, job creation, and social development, and how can it align with the country's strategic goals and initiatives?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  25%|██▌       | 20/80 [01:34<04:37,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What was the main reason for the high credit growth in Vietnam’s banking sector in 1Q22?\n",
      "transform quetstion:  What were the primary factors contributing to the significant increase in credit growth in Vietnam's banking sector during the first quarter of 2022, including any notable trends or regional differences? Could you provide details on the main drivers behind the high credit growth in Vietnam's banking sector during Q1 2022, such as changes in monetary policy, economic indicators, or government initiatives? What insights do you have on the relationship between interest rates and credit growth in Vietnam's banking sector for Q1 2022, including any observations from central banks, financial institutions, or industry reports? Was there a specific event, regulation, or external factor that played a significant role in fueling high credit growth in Vietnam's banking sector during the first quarter of 2022? Can you elaborate on how the Vietnamese government's economic stimulus packages and fiscal policies might have impacted credit growth in the banking sector during Q1 2022?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  26%|██▋       | 21/80 [01:40<04:47,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How did inflation expectations impact the Net Interest Margin (NIM) in 1Q22?\n",
      "transform quetstion:  What was the effect of inflation expectations on the Net Interest Margin (NIM) for financial institutions specifically, as measured by their quarterly earnings reports in 1Q22, and how did this impact their ability to maintain profitability during a period of rising interest rates?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  28%|██▊       | 22/80 [01:43<04:15,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What role does Artificial Intelligence (AI) play in assessing credit risk in the banking sector?\n",
      "transform quetstion:  How AI-powered machine learning algorithms are used by banks to assess individual and organizational creditworthiness, including factors such as loan-to-value ratio and credit history. What is the specific application of deep learning techniques in credit risk assessment for commercial and retail loans in the banking industry? Can you provide examples of how natural language processing (NLP) is used to extract relevant information from unstructured data sources, such as customer reviews and social media posts, to inform credit risk decisions in the financial sector? How do banks use AI-driven predictive modeling to identify potential credit risks, including the role of historical data, market trends, and external factors like economic conditions and regulatory changes? AI-powered machine learning algorithms deep learning techniques\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  29%|██▉       | 23/80 [01:48<04:23,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How has the retail banking sector in Vietnam changed since the banking crisis of 2011-2012?\n",
      "transform quetstion:  What were the key regulatory reforms implemented by the State Bank of Vietnam and the National Assembly to address the banking sector's vulnerabilities following the 2011-2012 crisis, and how have these measures impacted the retail banking landscape in Vietnam since then? Can you provide an analysis of the current state of the retail banking sector in Vietnam, including any notable mergers and acquisitions, changes in market share, or shifts in consumer behavior, in the aftermath of the 2011-2012 crisis? How has the implementation of Basel III and Basel IV regulations in Vietnam affected the retail banking sector, particularly with regards to capital requirements, liquidity ratios, and risk management practices, since the 2011-2012 crisis? What have been the primary drivers of growth or decline in the retail banking sector in Vietnam since 2011-2012, including changes in interest rates, competition from fintech providers, or shifts in consumer behavior towards digital banking?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  30%|███       | 24/80 [01:55<05:04,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How did the trend in Non-Performing Loans (NPLs) change in 1Q22, and what impact did COVID-19 have?\n",
      "transform quetstion:  Provide an analysis of the quarterly trend in Non-Performing Loans (NPLs) for January-March 2022, specifically highlighting any changes or shifts compared to previous quarters. Additionally, discuss how COVID-19 pandemic-related factors influenced NPL growth during this period. What was the specific impact of the COVID-19 pandemic on the Q1 2022 Non-Performing Loan (NPL) trend in [specific region/country]? Please provide data on the percentage change or absolute numbers for NPLs, as well as any relevant explanations or insights on how the pandemic affected lending habits and credit quality. Compare the Q1 2022 NPL trends with those from previous quarters of 2020-2022 to identify any changes in the overall trend. Also, investigate whether the COVID-19 pandemic played a role in altering the NPL landscape during this time, including its impact on [specific industry/sector].\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  31%|███▏      | 25/80 [02:00<04:44,  5.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What strategies are banks in Vietnam employing to manage liquidity during periods of high credit growth?\n",
      "transform quetstion:  What specific reserve requirements or liquidity buffers are Vietnamese banks implementing to mitigate the impact of rapid credit growth on their financial stability, and how have these measures been influenced by regulatory guidelines set by the State Bank of Vietnam? How have Vietnamese banks been adapting their liquidity management strategies to accommodate increasing credit growth in recent years, taking into account factors such as interest rate fluctuations, currency volatility, and macroeconomic conditions?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  32%|███▎      | 26/80 [02:05<04:35,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  AI đang được sử dụng như thế nào trong hệ thống quản lý giám sát video trong ngành ngân hàng để phòng ngừa gian lận?\n",
      "transform quetstion:  Ai được ứng dụng như thế nào trong các hệ thống quản lý giám sát video của ngành ngân hàng nhằm ngăn chặn và phát hiện hoạt động gian lận bằng cách phân tích hình ảnh và dữ liệu?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  34%|███▍      | 27/80 [02:07<03:48,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What challenges have arisen in the deposit growth of Vietnamese banks in 1Q22?\n",
      "transform quetstion:  What were the key challenges faced by Vietnamese banks in terms of deposit growth during the first quarter of 2022 (1Q22), including any notable trends, obstacles, or sector-specific issues? deposit growth key challenges trends obstacles sector-specific issues\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  35%|███▌      | 28/80 [02:11<03:31,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  AI góp phần cải thiện sự tương tác và sự hài lòng của khách hàng trong ngành ngân hàng như thế nào?\n",
      "transform quetstion:  AI là công nghệ có thể cải thiện giao dịch ngân hàng một cách nào, làm tăng khả năng tương tác tốt hơn với khách hàng, và đạt được mức độ hài lòng cao nhất? sự tương tác AI là công nghệ nào trong ngành ngân hàng có thể cải thiện sự tương tác giữa khách hàng và ngân hàng, làm tăng khả năng hài lòng của khách hàng?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  36%|███▋      | 29/80 [02:15<03:29,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the projections for banks' Non-Performing Loan (NPL) rates in the latter half of 2022 and into 2023?\n",
      "transform quetstion:  I am looking for data on the projected Non-Performing Loan (NPL) rates for banks globally, specifically in terms of the percentage of loans considered NPL by year-end [insert date], and how these projections are expected to evolve between Q4 2022 and Q1 2023. Additionally, I would like to know which regions or countries will likely see the largest increases in NPL rates during this time period, as well as any notable trends or factors driving these projections.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  38%|███▊      | 30/80 [02:20<03:34,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Hệ thống ngân hàng đóng vai trò gì trong việc phát triển kinh tế và xây dựng quốc gia?\n",
      "transform quetstion:  Hệ thống ngân hàng có vai trò quan trọng nào trong việc thúc đẩy tăng trưởng kinh tế, tạo ra cơ hội tài chính cho doanh nghiệp và cá nhân, cũng như đóng góp vào sự ổn định và phát triển bền vững của quốc gia? Thúc đẩy tăng trưởng kinh tế phát triển kinh tế Tạo ra cơ hội tài chính cho doanh nghiệp và cá nhân Đóng góp vào sự ổn định và phát triển bền vững của quốc gia\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  39%|███▉      | 31/80 [02:24<03:26,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What does a bank need to operate effectively?\n",
      "transform quetstion:  What are the essential functions and requirements for a bank to maintain operational efficiency, including financial management systems, customer service protocols, regulatory compliance, and personnel training? financial management systems customer service protocols operational efficiency maintenance of regulatory compliance personnel training operate effectively\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  40%|████      | 32/80 [02:27<03:11,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Vai trò của ngân hàng trong việc quản lý tài sản cá nhân là gì?\n",
      "transform quetstion:  Ngân hàng đóng vai trò quan trọng trong việc quản lý tài sản cá nhân bằng cách cung cấp các dịch vụ tài chính, bao gồm mở tài khoản tiết kiệm, đầu tư cổ phiếu, mua bảo hiểm, và tư vấn tài chính cá nhân. Tài chính cá nhân thường được quản lý bởi ngân hàng thông qua các dịch vụ như quản lý tài khoản tiết kiệm, đầu tư cổ phiếu, và mua bảo hiểm. Ngân hàng cũng cung cấp tư vấn tài chính cá nhân để giúp khách hàng quản lý tài sản của mình một cách hiệu quả.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  41%|████▏     | 33/80 [02:32<03:16,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tại sao ngân hàng có vai trò quan trọng trong việc chuyển đổi giá trị?\n",
      "transform quetstion:  Xác định vai trò cụ thể của ngân hàng trong quá trình chuyển đổi giá trị, bao gồm cả khía cạnh kinh tế, tài chính và xã hội. cụ thể vai trò quan trọng kinh tế tài chính sản phẩm xã hội\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  42%|████▎     | 34/80 [02:37<03:22,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How do banks help promote economic activity?\n",
      "transform quetstion:  What role do commercial banks play in promoting small business lending, employment creation, and regional economic development through their credit expansion policies, business loans, and investment initiatives? economic activity small business lending regional economic development credit expansion policies, business loans, and investment initiatives small business lending regional economic development\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  44%|████▍     | 35/80 [02:41<03:12,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  \"Banks Serve as Multipliers of Wealth\" nghĩa là gì?\n",
      "transform quetstion:  Giải thích cách các ngân hàng tăng cường tài sản và tạo ra lợi nhuận trong hệ thống tài chính hiện đại. Tìm hiểu về vai trò của ngân hàng trong việc tạo ra sức mạnh tài chánh và giải quyết vấn đề tài chính cho cá nhân và doanh nghiệp.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  45%|████▌     | 36/80 [02:44<02:54,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the role of banks in promoting the market?\n",
      "transform quetstion:  What are the specific ways in which banks contribute to the promotion of financial markets, including their role in setting interest rates, facilitating capital flows, and influencing monetary policy?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  46%|████▋     | 37/80 [02:47<02:39,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tại sao cần có một hệ thống tư pháp nhanh chóng và công bằng cho hoạt động của ngân hàng?\n",
      "transform quetstion:  \n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  48%|████▊     | 38/80 [02:52<02:45,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tại sao sự độc lập của ngân hàng trung ương lại quan trọng?\n",
      "transform quetstion:  Explaining the significance of central bank independence in maintaining monetary policy stability and promoting economic growth. The importance of central bank autonomy in setting interest rates and managing inflation, with examples from recent global financial crises. Discussing the role of central bank independence in ensuring fiscal discipline and preventing excessive government borrowing, with implications for macroeconomic stability. Monetary policy Inflation control Fiscal discipline Macroeconomic stability Interest rate setting\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  49%|████▉     | 39/80 [02:56<02:42,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Ngân hàng có vai trò gì trong việc cải thiện quản trị quốc gia?\n",
      "transform quetstion:  Thế giới ngân hàng và vai trò của nó trong việc hỗ trợ phát triển quản trị quốc gia, bao gồm cả các ví dụ cụ thể về cách ngân hàng đóng góp vào tăng cường tính minh bạch, hiệu quả và sự đồng bộ trong chính sách công. The world of banking and its role in supporting the development of national management, including specific examples of how banks contribute to enhancing transparency, efficiency, and coherence in public policy. Thế giới ngân hàng Hỗ trợ phát triển quản trị quốc gia Tăng cường tính minh bạch Hiệu quả và sự đồng bộ\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  50%|█████     | 40/80 [02:59<02:34,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Ngành bảo hiểm Việt Nam gặp phải những thách thức gì trong năm 2023?\n",
      "transform quetstion:  Giải pháp và thách thức ngành bảo hiểm Việt Nam đang đối mặt trong năm 2023 giải pháp thách thức đã đối mặt\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  51%|█████▏    | 41/80 [03:04<02:36,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How did the non-life insurance segment perform in 2023?\n",
      "transform quetstion:  What was the growth rate of the non-life insurance segment globally in 2023 compared to 2022, including any notable trends or changes in premium rates, claims frequency, and market share?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  52%|█████▎    | 42/80 [03:06<02:15,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Các loại hình bảo hiểm nhân thọ phổ biến nhất tại Việt Nam hiện nay là gì?\n",
      "transform quetstion:  Xác định các loại hình bảo hiểm nhân thọ phổ biến nhất và được sử dụng rộng rãi tại Việt Nam hiện tại. phổ biến nhất và được sử dụng rộng rãi at present\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  54%|█████▍    | 43/80 [03:10<02:12,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Which insurance company will lead in new premium revenue in 2023?\n",
      "transform quetstion:  Which insurance companies are expected to experience the largest growth in new premium revenue in 2023, according to recent market reports and industry trends? largest growth new premium revenue 2023 recent market reports and industry trends\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  55%|█████▌    | 44/80 [03:14<02:11,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Vai trò của đầu tư tài chính trong hoạt động của các công ty bảo hiểm là gì?\n",
      "transform quetstion:  \n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  56%|█████▋    | 45/80 [03:18<02:12,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Các doanh nghiệp bảo hiểm nhân thọ thay đổi danh mục đầu tư như thế nào từ năm 2015 đến năm 2022?\n",
      "transform quetstion:  Các doanh nghiệp bảo hiểm nhân thọ tại Việt Nam đã thực hiện thay đổi danh mục đầu tư nào từ năm 2015 đến cuối năm 2022, và những yếu tố dẫn đến những thay đổi này? Việt Nam from 2015 from năm 2015 đến cuối năm 2022 thay đổi danh mục đầu tư nào và những yếu tố dẫn đến những thay đổi này?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  57%|█████▊    | 46/80 [03:26<02:56,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the average claim rate for non-life insurance and which type of insurance has the highest claim rate?\n",
      "transform quetstion:  Can you provide the average annual claims per premium paid for non-life insurance in [specific country/region] over the past 5 years, as well as a comparison of claim rates across different types of non-life insurance, such as property damage, liability, and accident-only policies?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  59%|█████▉    | 47/80 [03:30<02:33,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tỷ lệ thâm nhập của bảo hiểm tại Việt Nam so với GDP năm 2023 là bao nhiêu và mục tiêu năm 2025 của chính phủ là gì?\n",
      "transform quetstion:  Tìm thông tin chi tiết về tỷ lệ thâm nhập của bảo hiểm tại Việt Nam trong năm 2023, bao gồm cả con số cụ thể và xu hướng tăng trưởng. Đồng thời, tìm hiểu mục tiêu chính phủ liên quan đến bảo hiểm cho dân cư trong năm 2025, bao gồm cả kế hoạch và chiến lược cụ thể. chi tiết năm 2023 xu hướng tăng trưởng đồng thời\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  60%|██████    | 48/80 [03:34<02:24,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  PVI có kết quả kinh doanh ra sao trong quý 1 năm 2024?\n",
      "transform quetstion:  PVI công ty mẹ có doanh thu và lợi nhuận ròng nào trong quý 1 năm 2024? Hoặc PVI có thông tin cập nhật về kết quả kinh doanh của công ty vào tháng 1 năm 2024 không?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  61%|██████▏   | 49/80 [03:41<02:46,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Which insurance segment will account for the largest proportion of BIC's revenue in 2024?\n",
      "transform quetstion:  What type of insurance segment (e.g. life, health, property & casualty) is expected to generate the highest revenue share for BIC Group's overall business in 2024, and what specific metrics or data points are used to support this forecast? insurance segment\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  62%|██████▎   | 50/80 [03:45<02:28,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the Global Monitoring Exercise (GME) as mentioned in the report?\n",
      "transform quetstion:  What is the purpose, scope, and objectives of the Global Monitoring Exercise (GME), and what are its key components or activities as described in the specific report I am referring to? specific report What are the main goals and deliverables of the Global Monitoring Exercise (GME), and how does it relate to [specific topic or area] in the report? [specific topic or area]\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  64%|██████▍   | 51/80 [03:49<02:13,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Môi trường kinh tế vĩ mô tác động như thế nào đến ngành bảo hiểm năm 2022?\n",
      "transform quetstion:  Mô hình kinh tế vĩ mô và các yếu tố ảnh hưởng trong năm 2022 đã làm thay đổi bao nhiêu trong lĩnh vực bảo hiểm? Ví dụ cụ thể như thế nào nó ảnh hưởng đến tốc độ tăng trưởng bảo hiểm, mức giá bảo hiểm, sự cạnh tranh giữa các công ty bảo hiểm Việt Nam? mô hình kinh tế vĩ mô tốc độ tăng trưởng bảo hiểm mức giá bảo hiểm  năm 2022 sự cạnh tranh giữa các công ty bảo hiểm Việt Nam\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  65%|██████▌   | 52/80 [03:53<02:05,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the main challenges related to liquidity risks in the insurance sector?\n",
      "transform quetstion:  What are the primary factors contributing to liquidity risk in the insurance sector, particularly in relation to policyholder payments, claims settlements, and reinsurer transactions? What strategies can insurance companies employ to mitigate these risks and maintain adequate liquidity levels during periods of high demand or market volatility?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  66%|██████▋   | 53/80 [03:57<01:57,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tầm quan trọng của rủi ro liên quan đến khí hậu đối với ngành bảo hiểm như đã thảo luận trong báo cáo là gì?\n",
      "transform quetstion:  rủi ro cụ thể liên quan đến biến đổi khí hậu rủi ro liên quan đến khí hậu số liệu và phương pháp phân tích\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  68%|██████▊   | 54/80 [04:02<01:53,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How is reinsurance playing a role in mitigating climate-related risks for insurers?\n",
      "transform quetstion:  What are the key strategies employed by reinsurers to mitigate climate-related risks for insurers, and how have recent regulatory developments impacted the industry's approach to sustainability in reinsurance? strategies employed sustainability in reinsurance\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  69%|██████▉   | 55/80 [04:06<01:48,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:   What are \"asset-intensive reinsurance\" agreements, and why are they significant?\n",
      "transform quetstion:  What are asset-intensive reinsurance contracts, specifically focusing on the types of assets being reinsured, the typical industries or sectors involved, and their implications for risk management and capital allocation in the reinsurance market? Asset-intensive Reinsurance contracts Types of assets being reinsured Industries or sectors involved Implications for risk management and capital allocation\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  70%|███████   | 56/80 [04:10<01:43,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Một số rủi ro liên quan đến việc tăng phân bổ tài sản thay thế của các công ty bảo hiểm là gì?\n",
      "transform quetstion:  Tìm kiếm thông tin về những rủi ro cụ thể mà các công ty bảo hiểm có thể gặp phải khi tăng phân bổ tài sản thay thế, bao gồm cả rủi ro tài chính, tín dụng, thanh khoản, và rủi ro quy định pháp luật.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  71%|███████▏  | 57/80 [04:14<01:38,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the key metrics used to assess the climate-related risks of sovereign debt instruments?\n",
      "transform quetstion:  What are the primary macroeconomic indicators (e.g. GDP growth rate, inflation rate) and risk assessment frameworks (e.g. stress testing, scenario planning) commonly used to evaluate the climate-related vulnerability of sovereign debt instruments, and how do these metrics interact with each other?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  72%|███████▎  | 58/80 [04:18<01:30,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  How do changes in the solvency ratios of insurers impact their ability to absorb macroeconomic shocks?\n",
      "transform quetstion:  What are the specific effects of changes in solvency ratios on an insurer's capital adequacy, risk tolerance, and ability to withstand macroeconomic shocks, such as economic downturns or regulatory stress tests? specific effects such as economic downturns or regulatory stress tests capital adequacy risk tolerance withstand\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  74%|███████▍  | 59/80 [04:22<01:26,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the supervisory measures suggested to address the risks related to cross-border asset-intensive reinsurance?\n",
      "transform quetstion:  What are the specific regulatory and operational measures recommended by international insurance organizations (such as Solvency II) and industry experts for managing the unique risks associated with cross-border asset-intensive reinsurance, including asset liability management, capital adequacy requirements, and governance structures? supervisory measures regulatory and operational measures\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  75%|███████▌  | 60/80 [04:26<01:20,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the primary applications of particle accelerators in the medical field?\n",
      "transform quetstion:  What are the primary medical treatments or procedures enabled by particle accelerators, such as proton therapy, cancer radiation treatment, and radioisotope production, and what specific diseases or conditions do they target?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  76%|███████▋  | 61/80 [04:29<01:09,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Sự khác biệt giữa liệu pháp hadron và xạ trị cổ điển là gì?\n",
      "transform quetstion:  So sánh và phân tích sự khác biệt giữa liệu pháp hadron và xạ trị cổ điển trong điều trị bệnh ung thư. Giải thích chi tiết về các yếu tố khác nhau của liệu pháp hadron so với xạ trị cổ điển, bao gồm cả lợi ích và hạn chế. Tìm hiểu về phân biệt kỹ thuật giữa liệu pháp hadron và xạ trị cổ điển, đặc biệt là trong trường hợp điều trị bệnh ung thư. So sánh hiệu quả của liệu pháp hadron với xạ trị cổ điển đối với các bệnh lý khác nhau, bao gồm cả ung thư. Giải thích cách thức hoạt động của liệu pháp hadron và xạ trị cổ điển, cũng như sự khác biệt cơ bản giữa hai phương pháp này. so sánh và phân tích yếu tố khác nhau phân biệt kỹ thuật giải thích chi tiết về các yếu tố khác nhau tìm hiểu về phân biệt kỹ thuật\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  78%|███████▊  | 62/80 [04:34<01:15,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the Bragg peak, and why is it important in hadrontherapy?\n",
      "transform quetstion:  Explain the concept of the Bragg peak in physics, its relation to radiation energy deposition, and provide examples of how it is utilized in Hadrontherapy to optimize treatment outcomes. What are the characteristics of the Bragg peak in proton therapy, particularly in relation to particle range and tissue sparing? How does this phenomenon impact the design of Hadrontherapy treatment plans?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  79%|███████▉  | 63/80 [04:38<01:08,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Hai kỹ thuật chẩn đoán hình ảnh chính sử dụng đồng vị phóng xạ là gì?\n",
      "transform quetstion:  Hiện nay, các phương pháp chẩn đoán hình ảnh chính sử dụng đồng vị phóng xạ nào? chẩn đoán hình ảnh chính hình ảnh\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  80%|████████  | 64/80 [04:41<01:01,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Why is Technetium-99m significant in nuclear medicine?\n",
      "transform quetstion:  What are the medical applications of Technetium-99m (Tc-99m) in diagnostic imaging for cancer detection, particularly in oncology and cardiology? How does Technetium-99m (Tc-99m) contribute to nuclear medicine procedures, such as bone scans, cardiac perfusion imaging, and tumor localization in cancer patients? What are the advantages of using Technetium-99m (Tc-99m) in nuclear medicine over other isotopes, and what is its optimal dosing for effective diagnostic imaging? Can you provide examples of specific clinical applications of Technetium-99m (Tc-99m) in medical research, including cancer treatment monitoring, neuroimaging, and cardiovascular disease detection? How does the half-life, radiation characteristics, and chemical properties of Technetium-99m (Tc-99m) influence its suitability for various nuclear medicine procedures and diagnostic protocols?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  81%|████████▏ | 65/80 [04:46<01:01,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the role of synchrotrons and cyclotrons in hadrontherapy?\n",
      "transform quetstion:  What specific medical applications and benefits can be achieved through the use of synchrotrons and cyclotrons in hadrontherapy? How do the principles of synchrotron radiation and cyclotron acceleration contribute to the therapeutic effects of hadron beams in cancer treatment? What are the key differences between the physics of synchrotron radiation and cyclotron acceleration, and how do these impact the design and operation of hadrontherapy facilities? Can you provide an overview of the current research and development efforts focused on improving the beam quality, intensity, and delivery precision of synchrotron-based and cyclotron-based hadron beams for cancer treatment?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  82%|████████▎ | 66/80 [04:51<01:02,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What is the significance of Linear Energy Transfer (LET) in radiotherapy?\n",
      "transform quetstion:  What is the relationship between Linear Energy Transfer (LET) and biological damage in radiation therapy, specifically in terms of its impact on tumor cells vs. healthy tissue? Can you provide an explanation of how Linear Energy Transfer (LET) affects the efficacy of radiotherapy in treating cancerous tumors, including any limitations or potential side effects? How does Linear Energy Transfer (LET) compare to other radiation therapy parameters, such as dose rate or beam quality, in determining treatment outcomes for patients with [specific type of cancer]? What is the current understanding of Linear Energy Transfer (LET) in radiotherapy, including any recent studies or clinical trials that have investigated its effects on [specific biological process or patient outcome]?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  84%|████████▍ | 67/80 [04:56<01:00,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Kỹ thuật phân phối chùm tia thụ động hoạt động như thế nào trong liệu pháp hadron?\n",
      "transform quetstion:  Tham khảo các phương pháp và kỹ thuật phân phối chùm tia thụ động trong liệu pháp hadronic, bao gồm cả các phương pháp điều chỉnh tần số, cường độ và thời gian của chùm tia, cũng như cách chúng ảnh hưởng đến hiệu quả của liệu pháp hadron. tham khảo phương pháp và kỹ thuật phân phối chùm tia thụ động kỹ thuật phân phối chùm tia lieu pháp hadronic điều chỉnh tần số, cường độ và thời gian của chùm tia\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  85%|████████▌ | 68/80 [05:01<00:55,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the challenges of using synchrotrons for hadrontherapy?\n",
      "transform quetstion:  What are the primary technical, biological, and safety challenges associated with using synchrotron radiation in hadrontherapy treatments, including radiation dosing accuracy, tumor penetration depth, and potential side effects on surrounding healthy tissue? hadrontherapy primary technical, biological, and safety challenges\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  86%|████████▋ | 69/80 [05:05<00:50,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What future developments in accelerator technology are highlighted for hadrontherapy?\n",
      "transform quetstion:  Future advancements in high-energy particle accelerators for cancer treatment (hadrontherapy), including any notable breakthroughs or innovations in acceleration techniques, beam quality, and radiation dose delivery, as well as their potential impact on improving patient outcomes. high-energy particle accelerators cancer treatment (hadrontherapy) acceleration techniques beam quality radiation dose delivery\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  88%|████████▊ | 70/80 [05:09<00:42,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tại sao các bác sĩ lâm sàng cần phải học phương pháp nghiên cứu khoa học?\n",
      "transform quetstion:  Phạm vi và lợi ích của việc sử dụng phương pháp nghiên cứu khoa học trong đào tạo và thực hành y lâm sàng cho các bác sĩ. Why do doctors need to learn scientific research methods for their clinical practice? phương pháp nghiên cứu khoa học nghiên cứu khoa học đào tạo và thực hành y lâm sàng cho các bác sĩ\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  89%|████████▉ | 71/80 [05:13<00:37,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  \"Dịch tễ học\" đóng vai trò gì trong nghiên cứu y khoa?\n",
      "transform quetstion:  Tìm hiểu về dịch tễ học và vai trò của nó trong nghiên cứu y khoa, bao gồm các phương pháp và kỹ thuật được sử dụng để phân tích và đánh giá rủi ro và tác động của bệnh tật đối với người bệnh. dịch tễ học dịch tễ học phương pháp và kỹ thuật được sử dụng phân tích và đánh giá rủi ro\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  90%|█████████ | 72/80 [05:17<00:34,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the basic steps to conducting a medical study?\n",
      "transform quetstion:  What are the key components of an Institutional Review Board (IRB) approval process for human subjects in clinical trials, including the necessary steps for designing a randomized controlled trial, conducting informed consent procedures, maintaining participant confidentiality, and ensuring data quality and safety? Institutional Review Board human subjects clinical trials IRB approval process randomized controlled trial\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  91%|█████████▏| 73/80 [05:21<00:29,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Why is it necessary to search the literature before conducting research?\n",
      "transform quetstion:  \n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  92%|█████████▎| 74/80 [05:26<00:26,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Nghiên cứu mô tả là gì và vai trò của nó trong y khoa?\n",
      "transform quetstion:  Xin cho tôi một bản tóm tắt nghiên cứu về định nghĩa và tầm quan trọng của nghiên cứu mô tả (description study) trong lĩnh vực y khoa. Tìm kiếm thông tin chi tiết về phân loại nghiên cứu mô tả, cách thức thực hiện và ứng dụng của nó trong các phương pháp điều trị y khoa hiện đại. Làm rõ vai trò của nghiên cứu mô tả (description study) trong việc đánh giá hiệu quả của các thuốc và phương pháp điều trị mới trong y khoa, bao gồm cả hạn chế và lợi ích của nó.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  94%|█████████▍| 75/80 [05:30<00:20,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Các loại sai lệch (bias) có thể gặp trong nghiên cứu y khoa là gì?\n",
      "transform quetstion:  Các loại sai lệch (bias) có thể xảy ra trong nghiên cứu y khoa như thế nào và chúng có ảnh hưởng đến tính xác thực của kết quả nghiên cứu? Tìm thông tin về các loại bias phổ biến nhất trong nghiên cứu y khoa, bao gồm cả bias do người thực hiện, bias do dữ liệu, và cách để giảm thiểu chúng.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  95%|█████████▌| 76/80 [05:35<00:17,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Nhiễu (confounding) là gì và tại sao nó quan trọng trong nghiên cứu?\n",
      "transform quetstion:  Phân tích về sự xuất hiện của sự nhầm lẫn (confounding) trong nghiên cứu y học và xã hội, đồng thời giải thích vai trò và tác động quan trọng của nó đối với tính xác thực và đáng tin cậy của kết quả nghiên cứu. Nhiễu confounding sự xuất hiện của sự nhầm lẫn nghiên cứu y học và xã hội phân tích\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  96%|█████████▋| 77/80 [05:39<00:13,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Thế nào là một nghiên cứu cắt ngang và mục tiêu của nó?\n",
      "transform quetstion:  nghiên cứu cắt ngang mục tiêu nghiên cứu cắt ngang nghiên cứu cắt ngang trong quản lý dự án cắt ngang nghiên cứu tại Việt Nam lợi ích của cắt ngang trong phân tích dữ liệu Thế nào là một nghiên cứu cắt ngang và mục tiêu cụ thể của nó trong quản lý dự án? Các yếu tố quan trọng quyết định mục tiêu của một nghiên cứu cắt ngang? Kết quả nghiên cứu cắt ngang liên quan đến gì? Tôi muốn tìm hiểu về nghiên cứu cắt ngang và cách xác định mục tiêu của nó trong khoa học quản lý. Nghiên cứu cắt ngang là gì? Và tại sao chúng ta cần thiết lập mục tiêu cho các nghiên cứu này? Một nghiên cứu cắt ngang là một phương pháp nghiên cứu tập trung vào việc phân tích dữ liệu ở một điểm cụ thể trong quá trình thực hiện dự án. Mục tiêu của nghiên cứu cắt ngang là hiểu rõ hơn về các yếu tố ảnh hưởng đến hiệu suất và quyết định của tổ chức. Các mục tiêu chính của nghiên cứu cắt ngang bao gồm: (1) nhận thức, (2) hành vi, và (3) kết quả đạt được trong dự án. Các mục tiêu này giúp cho các nhà quản lý có cái nhìn tổng thể hơn về hiệu quả của dự án và đưa ra quyết định sáng suốt. Nghiên cứu cắt ngang thường áp dụng ở các lĩnh vực như quản lý dự án, kinh doanh, và giáo dục để đánh giá hiệu suất thực hiện công việc.\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  98%|█████████▊| 78/80 [05:46<00:10,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  What are the data collection methods in cross-sectional research?\n",
      "transform quetstion:  \n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs:  99%|█████████▉| 79/80 [05:50<00:04,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original question:  Tại sao cần đánh giá có hệ thống các bài báo khoa học khi làm nghiên cứu?\n",
      "transform quetstion:  Xin hãy giải thích lý do quan trọng tại sao việc đánh giá và xác định có các bài báo khoa học trong quá trình nghiên cứu là cần thiết, bao gồm cả mục đích và lợi ích của việc này?\n",
      "Vector searched `VectorDB_1024`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Q&A pairs: 100%|██████████| 80/80 [05:54<00:00,  4.43s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cecdc63c8314d5a81ce3cbd99936480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.133 MB of 0.133 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_faithfulness_score</td><td>0.35</td></tr><tr><td>avg_hit_rate_score</td><td>0.3625</td></tr><tr><td>avg_semantic_score</td><td>0.78664</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">experiment-2024-12-27 00:39:15</strong> at: <a href='https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC/runs/fhkk9t1d' target=\"_blank\">https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC/runs/fhkk9t1d</a><br/> View project at: <a href='https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC' target=\"_blank\">https://wandb.ai/thainhattan26012003-llm/FSG.FTI.G2.POC</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241227_003915-fhkk9t1d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title # 🧪 Moment of truth\n",
    "\n",
    "# Run experiments\n",
    "import datetime\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"FSG.FTI.G2.POC\",\n",
    "    name=f\"experiment-{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    # notes=desc,\n",
    "    config={\n",
    "        \"model\": \"Llama3.2\",\n",
    "        \"encoder\": \"multi-qa-mpnet-base-dot-v1\",\n",
    "        \"use_qdrant_local\": Use_Qdrant_Local,\n",
    "        \"conf\": conf\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluate_prompt = \"\"\"\n",
    "Evaluate the faithfulness. Check if the answer is related or answered by the provided context.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "Return `TRUE` if the answer is faithful and `FALSE` otherwise.\n",
    "\"\"\"\n",
    "\n",
    "ai_answers = []\n",
    "semantic_scores = []\n",
    "faithfulness_scores = []\n",
    "hit_rate_scores = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating Q&A pairs\"):\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "    title = row['Title']\n",
    "\n",
    "    ai_answer, filename, context = evaluate_function(question)\n",
    "    ai_answers.append(ai_answer)\n",
    "\n",
    "    # Semantic Similarity Score\n",
    "    embedding = model.encode([answer, ai_answer])\n",
    "    semantic_similarity = embedding[0].dot(embedding[1]) / (\n",
    "        np.linalg.norm(embedding[0]) * np.linalg.norm(embedding[1])\n",
    "    )\n",
    "    semantic_scores.append(semantic_similarity)\n",
    "\n",
    "    # Faithfulness Score (example using LangChain's ollama for scoring)\n",
    "    prompt = evaluate_prompt.format(question=question, answer=ai_answer, context=context)\n",
    "    faithfulness_score = model_gemini.invoke(prompt)  # Replace with actual scoring method\n",
    "\n",
    "    if 'TRUE' in faithfulness_score.content:\n",
    "        faithfulness_scores.append(1)\n",
    "    elif 'FALSE' in faithfulness_score.content:\n",
    "        faithfulness_scores.append(0)\n",
    "    else:\n",
    "        faithfulness_scores.append(0)\n",
    "\n",
    "    # Hit Rate Score\n",
    "    hit_rate_score = 1 if filename.lower() in title.lower() else 0\n",
    "    hit_rate_scores.append(hit_rate_score)\n",
    "\n",
    "# Calculate average scores\n",
    "avg_semantic_score = sum(semantic_scores) / len(semantic_scores) if semantic_scores else 0\n",
    "avg_faithfulness_score = sum(faithfulness_scores) / len(faithfulness_scores) if faithfulness_scores else 0\n",
    "avg_hit_rate_score = sum(hit_rate_scores) / len(hit_rate_scores) if hit_rate_scores else 0\n",
    "\n",
    "# Save the metrics\n",
    "run.summary[\"avg_semantic_score\"] = avg_semantic_score\n",
    "run.summary[\"avg_faithfulness_score\"] = avg_faithfulness_score\n",
    "run.summary[\"avg_hit_rate_score\"] = avg_hit_rate_score\n",
    "table = wandb.Table(columns=[\"Question\", \"Expected\", \"Title\", \"Answer\", \"Semantic Score\", \"Faithfulness Score\", \"Hit Rate Score\"])\n",
    "for i, (_, row) in enumerate(df.iterrows()):\n",
    "    table.add_data(row['Question'], row['Answer'], row['Title'], ai_answers[i], semantic_scores[i], faithfulness_scores[i], hit_rate_scores[i])\n",
    "wandb.log({\"table_key\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "query_rewrite_template = \"\"\"You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system. \n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "Rewritten query:\"\"\"\n",
    "\n",
    "query_rewrite_prompt = PromptTemplate(\n",
    "    input_variables=[\"original_query\"],\n",
    "    template=query_rewrite_template\n",
    ")\n",
    "\n",
    "# Create an LLMChain for query rewriting\n",
    "query_rewriter = query_rewrite_prompt | ollama\n",
    "\n",
    "def rewrite_query(original_query):\n",
    "    response = query_rewriter.invoke(original_query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What are the impacts of missing for imputation time series?\n",
      "\n",
      "Rewritten query: Here is a rewritten version of the original query:\n",
      "\n",
      "\"What are the specific effects on time series accuracy and reliability when using missing value imputation in datasets with varying levels of temporal aggregation (e.g. daily, weekly, monthly), and how do these impacts differ depending on the type of imputation method used (e.g. mean/median/mode, interpolation/slope-based)?\"\n",
      "\n",
      "This rewritten query is more specific, detailed, and likely to retrieve relevant information because it:\n",
      "\n",
      "* Specifies the context of time series data\n",
      "* Identifies the potential impacts on accuracy and reliability\n",
      "* Requests information about the effects of imputation methods with varying levels of temporal aggregation\n",
      "* Includes specific details about the types of imputation methods used\n",
      "\n",
      "By making these changes, the rewritten query is more likely to retrieve relevant information from a search engine or database, such as research papers or technical articles.\n"
     ]
    }
   ],
   "source": [
    "original_query = \"What are the impacts of missing for imputation time series?\"\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"Original query:\", original_query)\n",
    "print(\"\\nRewritten query:\", rewritten_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the specific effects on time series accuracy and reliability when using missing value imputation in datasets with varying levels of temporal aggregation (e.g. daily, weekly, monthly), and how do these impacts differ depending on the type of imputation method used (e.g. mean/median/mode, interpolation/slope-based)?\n"
     ]
    }
   ],
   "source": [
    "matches = re.findall(r'\"(.*?)\"', rewritten_query) \n",
    "\n",
    "\n",
    "print(\" \".join(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import getpass\n",
    "import os\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "if \"COHERE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")\n",
    "    \n",
    "reranker = CohereRerank(model=\"rerank-english-v3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "docs = [\n",
    "    \"Carson City is the capital city of the American state of Nevada.\",\n",
    "    \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.\",\n",
    "    \"Capitalization or capitalisation in English grammar is the use of a capital letter at the start of a word. English usage varies from capitalization in other languages.\",\n",
    "    \"Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district.\",\n",
    "    \"Capital punishment has existed in the United States since beforethe United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\",\n",
    "]\n",
    "\n",
    "response = co.rerank(\n",
    "    model=\"rerank-v3.5\",\n",
    "    query=\"What is the capital of the United States?\",\n",
    "    documents=docs,\n",
    "    top_n=3,\n",
    ")\n",
    "print(response.results[0].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "\n",
    "def generate_answer_with_llama(context, question):\n",
    "    template = f\"\"\"\n",
    "    You are an AI assistant. Use the provided context to answer the question accurately.\n",
    "    Return max 200 tokens, no fluff, just answer based on context retrieve\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | ollama\n",
    "    response = chain.invoke({\"question\": context})\n",
    "    return response\n",
    "\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "\n",
    "def re_ranking_cohere(query, docs, top_n=3, model=\"rerank-v3.5\"):\n",
    "    response = co.rerank(model=model, query=query, documents=docs, top_n=top_n)\n",
    "    return response\n",
    "\n",
    "\n",
    "def evaluate_function(question: str):\n",
    "\n",
    "    question_vector = embed(question)\n",
    "    \n",
    "    retrieve_db = vectordb_provider.search_vector(qdrant_name, question_vector, limit=3)\n",
    "    retrieve_db = [docs.payload['content'] for docs in retrieve_db]\n",
    "\n",
    "    re_rank = re_ranking_cohere(query=question, docs = retrieve_db)\n",
    "    \n",
    "    re_rank_index = re_rank.results[0].index\n",
    "    \n",
    "    response = generate_answer_with_llama(retrieve_db[re_rank_index], question)\n",
    "    \n",
    "    # filename = retrieve_db[re_rank_index].payload['file_name']\n",
    "    filename = \"test.pdf\" \n",
    "    \n",
    "    context = retrieve_db\n",
    "    \n",
    "    return (\n",
    "        response,\n",
    "        filename,\n",
    "        context\n",
    "    )\n",
    "# Login to Weight & Bias\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test single - Use this before running automatic test\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "# evaluate_function(\"What is the role of banks in promoting the market?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom configurations\n",
    "conf = {\n",
    "    \"distance\": \"COSINE\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # 🧪 Moment of truth\n",
    "\n",
    "# Run experiments\n",
    "import datetime\n",
    "import random\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"FSG.FTI.G2.POC\",\n",
    "    name=f\"experiment-{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    # notes=desc,\n",
    "    config={\n",
    "        \"model\": ollama,\n",
    "        \"encoder\": \"multi-qa-mpnet-base-dot-v1\",\n",
    "        \"use_qdrant_local\": Use_Qdrant_Local,\n",
    "        \"conf\": conf\n",
    "    }\n",
    ")\n",
    "\n",
    "evaluate_prompt = \"\"\"\n",
    "Evaluate the faithfulness. Check if the answer is related or answered by the provided context.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "Return `TRUE` if the answer is faithful and `FALSE` otherwise.\n",
    "\"\"\"\n",
    "\n",
    "ai_answers = []\n",
    "semantic_scores = []\n",
    "faithfulness_scores = []\n",
    "hit_rate_scores = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating Q&A pairs\"):\n",
    "    question = row['Question']\n",
    "    answer = row['Answer']\n",
    "    title = row['Title']\n",
    "\n",
    "    ai_answer, filename, context = evaluate_function(question)\n",
    "    ai_answers.append(ai_answer)\n",
    "\n",
    "    # Semantic Similarity Score\n",
    "    embedding = model.encode([answer, ai_answer])\n",
    "    semantic_similarity = embedding[0].dot(embedding[1]) / (\n",
    "        np.linalg.norm(embedding[0]) * np.linalg.norm(embedding[1])\n",
    "    )\n",
    "    semantic_scores.append(semantic_similarity)\n",
    "\n",
    "    # Faithfulness Score (example using LangChain's ollama for scoring)\n",
    "    prompt = evaluate_prompt.format(question=question, answer=ai_answer, context=context)\n",
    "    faithfulness_score = ollama.invoke(prompt)  # Replace with actual scoring method\n",
    "    if 'TRUE' in faithfulness_score:\n",
    "        faithfulness_scores.append(1)\n",
    "    elif 'FALSE' in faithfulness_score:\n",
    "        faithfulness_scores.append(0)\n",
    "    else:\n",
    "        faithfulness_scores.append(0)\n",
    "\n",
    "    # Hit Rate Score\n",
    "    hit_rate_score = 1 if filename.lower() in title.lower() else 0\n",
    "    hit_rate_scores.append(hit_rate_score)\n",
    "\n",
    "# Calculate average scores\n",
    "avg_semantic_score = sum(semantic_scores) / len(semantic_scores) if semantic_scores else 0\n",
    "avg_faithfulness_score = sum(faithfulness_scores) / len(faithfulness_scores) if faithfulness_scores else 0\n",
    "avg_hit_rate_score = sum(hit_rate_scores) / len(hit_rate_scores) if hit_rate_scores else 0\n",
    "\n",
    "# Save the metrics\n",
    "run.summary[\"avg_semantic_score\"] = avg_semantic_score\n",
    "run.summary[\"avg_faithfulness_score\"] = avg_faithfulness_score\n",
    "run.summary[\"avg_hit_rate_score\"] = avg_hit_rate_score\n",
    "table = wandb.Table(columns=[\"Question\", \"Expected\", \"Title\", \"Answer\", \"Semantic Score\", \"Faithfulness Score\", \"Hit Rate Score\"])\n",
    "for i, (_, row) in enumerate(df.iterrows()):\n",
    "    table.add_data(row['Question'], row['Answer'], row['Title'], ai_answers[i], semantic_scores[i], faithfulness_scores[i], hit_rate_scores[i])\n",
    "wandb.log({\"table_key\": table})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def multi_query(question):\n",
    "    template = f\"\"\"\n",
    "    You are an AI language model asssistant. Your task is to generate 3 different versions of the given user question to retrieve relevant documents from a vector database.\n",
    "    By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Original question : {question}\n",
    "    \n",
    "    Your response should be just 3 questions and nothing else, based on these form \n",
    "    [questions1, question2, question3]\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | ollama\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    return response\n",
    "\n",
    "    # response = model.invoke(prompt).strip()  # Generate text from the model\n",
    "    # questions = response.split(\"\\n\")  # Split the response into lines\n",
    "    # return [q.strip() for q in questions if q.strip()]  # Clean and return as a list\n",
    "\n",
    "gen = multi_query(\"What LLM work?\")\n",
    "gen = response_data = json.loads(response)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def generate_questions_with_ollama(input_question: str):\n",
    "    # Define the prompt\n",
    "    prompt = f\"\"\"\n",
    "    Generate 3 new questions based on the input question. Provide the output in the following JSON format:\n",
    "    \n",
    "    {{\n",
    "        \"generated_questions\": [\n",
    "            \"Question 1\",\n",
    "            \"Question 2\",\n",
    "            \"Question 3\"\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Input Question: \"{input_question}\"\n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the Ollama CLI\n",
    "    try:\n",
    "        # Replace `ollama run` with the actual command for running your local model\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3:8b\"],  # Replace with your specific model name\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True\n",
    "        )\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        response = result.stdout.strip()\n",
    "        response_data = json.loads(response)  # Ensure the response is valid JSON\n",
    "        \n",
    "        # Extract and return the generated questions\n",
    "        questions = response_data.get(\"generated_questions\", [])\n",
    "        return questions\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Failed to parse JSON: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "input_question = \"What are the benefits of hybrid search?\"\n",
    "generated_questions = generate_questions_with_ollama(input_question)\n",
    "\n",
    "print(\"Generated Questions:\")\n",
    "for idx, question in enumerate(generated_questions, start=1):\n",
    "    print(f\"{idx}: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query(question):\n",
    "    template = f\"\"\"\n",
    "    You are an AI language model asssistant. Your task is to generate 3 different versions of the given user question to retrieve relevant documents from a vector database.\n",
    "    By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.\n",
    "    Original question : {question}\n",
    "    \n",
    "    Your response should be a json of the just the 3 questions and nothing else\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | ollama\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    return response\n",
    "\n",
    "def get_top_n_results(results, n=3):\n",
    "    return sorted(results, key=lambda x: x['score'], reverse=True)[:n]\n",
    "\n",
    "def evaluate_function(question: str):\n",
    "    multi_ques = multi_query(question)\n",
    "    \n",
    "    \n",
    "\n",
    "    retrieve = query_qdrant(question, collection_name=qdrant_name, top_k=5)\n",
    "    \n",
    "    top_3_results = get_top_n_results(retrieve, n=3)\n",
    "    retrieve_db = \" \".join(result['content'] for result in top_3_results)\n",
    "    response = generate_answer_with_llama(retrieve_db, question)\n",
    "    \n",
    "    filename = [result['file_name'] for result in top_3_results][0]\n",
    "    \n",
    "    context = [result['content'] for result in top_3_results]\n",
    "    \n",
    "    # retrieve_db = [cont.payload['content'] for cont in retrieve][0]\n",
    "    # retrieve_db = \" \".join(retrieve_db)\n",
    "    # filename = [name.payload['file_name'] for name in retrieve][0]\n",
    "    \n",
    "    return (\n",
    "        response,\n",
    "        filename,\n",
    "        context\n",
    "    )\n",
    "# Login to Weight & Bias\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "def generate_multi_queries(question: str, model_name: str = \"t5-small\", num_queries: int = 3):\n",
    "    \"\"\"\n",
    "    Generate multiple query variations from a single question using T5.\n",
    "\n",
    "    Args:\n",
    "        question (str): Input question to generate variations.\n",
    "        model_name (str): Pre-trained model to use for query generation. Defaults to 't5-small'.\n",
    "        num_queries (int): Number of query variations to generate. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        list: Generated query variations.\n",
    "    \"\"\"\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Prepare input for T5 (specific prefix for question generation task)\n",
    "    input_text = f\"generate queries: {question}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate multiple outputs\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        num_return_sequences=num_queries,\n",
    "        num_beams=num_queries,  # Ensure diversity\n",
    "        max_length=50,          # Adjust depending on expected query length\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode generated queries\n",
    "    queries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Example usage\n",
    "question = \"What are the benefits of hybrid search in Qdrant?\"\n",
    "queries = generate_multi_queries(question, num_queries=5)\n",
    "\n",
    "print(\"Original Question:\", question)\n",
    "print(\"Generated Queries:\")\n",
    "for idx, query in enumerate(queries, start=1):\n",
    "    print(f\"{idx}: {query}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
